{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 卷积层\n",
    "## 从全连接到卷积\n",
    "- 平移不变性\n",
    "- 局部性\n",
    "- 二维交叉相关/卷积\n",
    "$\\rightarrow$\n",
    "<img src=\"https://s2.loli.net/2022/02/10/VpnHAMmKR2qxEar.png\" width=70%>\n",
    "- 对全连接层使用平移不变性和局部性得到卷积层\n",
    "<img src=\"https://s2.loli.net/2022/02/10/jlrhCisBmXQWOM8.png\" width=50%>\n",
    "\n",
    "## 卷积层\n",
    "- 边缘检测、锐化、高斯模糊\n",
    "- 交叉相关vs卷积：由于对称性因此使用中没有区别\n",
    "  - 一维与三维交叉相关\n",
    "- 核矩阵和偏移：\n",
    "  - 输入与和核矩阵进行交叉相关，加上偏移得到输出\n",
    "  - 可学习\n",
    "\n",
    "## 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 卷积函数定义:矩阵二维互相关运算\n",
    "def conv2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros([X.shape[0] - h + 1, X.shape[1] - w + 1]) # 这里不只是维度，更要有数值信息0\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()   # 居然漏了sum()，太可恶了\n",
    "            \n",
    "    return Y\n",
    "\n",
    "# 类定义\n",
    "class Conv2D(nn.Module):\n",
    "    def _init_(self, kernel_size):\n",
    "        super()._init_()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return conv2d(X, self.weight) + self.bias\n",
    "\n",
    "# 简单使用应用    \n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "lr = 3e-2\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    loss = (Y - Y_hat)**2     ## 梯度清空的操作对象是类\n",
    "    conv2d.zero_grad()\n",
    "    loss.sum().backward()       ## sum()\n",
    "    # Y_hat = Y_hat - lr * conv2d.grad   ## [:]使用 # 我是脑残\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'batch {i + 1}, loss {loss.sum():.3f}')\n",
    "    \n",
    "\n",
    "# ps备注\n",
    "# *是矩阵按位相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 感受野越大越好？\n",
    "  - 此思路与隐藏层的深度和宽度思想有关\n",
    "- 损失函数曲线下降抖动剧烈？\n",
    "  - batch_size或者lr增大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 卷积层里的填充和步幅\n",
    "## 填充和步幅\n",
    "- 填充padding\n",
    "  - 常用padding：$$p_h = k_h -1, p_w = k_w - 1$$\n",
    "  - 输出维度：$$(n_h - k_h + 1 + p_h, n_w - k_w + 1 + p_w)$$\n",
    "\n",
    "- 步幅stride\n",
    "<img src=\"https://s2.loli.net/2022/02/10/HTBm1tQfDSb3rv4.png\" width=70%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 注意：padding参数传入是两侧都填充还是单侧填充\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1, 1) + X.shape)    # 这东西可以增加维度\n",
    "    Y = conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 超参数中重要程度：核大小 > 填充 > 步幅\n",
    "- 卷积核边长一般选奇数\n",
    "- 过拟合：验证集较好情况下可以很好避免过拟合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 卷积层里的多输入多输出通道\n",
    "## 多输入多输出通道\n",
    "- 多个输入通道\n",
    "   - 彩色图片RGB --> 灰度 丢失信息 \n",
    "   - 输入$\\textbf{X}$,核$\\textbf{W}$,输出$\\textbf{Y}$--通道累加输出\n",
    "- 多输出通道\n",
    "   - 输入$X:c_i×n_h×n_w$\n",
    "   - 核$W:c_o×c_i×k_h×k_w$\n",
    "   - 输出$Y: c_o×m_h×m_w$\n",
    "- 多输入和多输出通道\n",
    "   - 输出：识别特定模式\n",
    "   - 输入：输入通道核识别并组合输入中的模式\n",
    "- 1×1 卷积层\n",
    "  - 不识别空间，只融合通道\n",
    "  - 相当于输入形状为$n_hn_w × c_i$，权重为$c_0 × c_i$的全连接层\n",
    "  \n",
    "## 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import d2l.torch as d2l\n",
    "\n",
    "# 多通道输入\n",
    "def corr2d_multi_in(X, K):\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))  \n",
    "\n",
    "# 多输出\n",
    "# 考虑物理意义，输出通道模式不同\n",
    "def corr2d_multi_in_out(X, K):\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n",
    "\n",
    "K = torch.stack((K, K + 1, K + 2))\n",
    "\n",
    "# 1*1 卷积\n",
    "def corr2d_multi_in_out_1multi1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))\n",
    "\n",
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1multi1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1, Y2).sum()) < 1e-6\n",
    "\n",
    "## 简洁调用\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2) #第一二个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 1.区别一下这些矩阵运算\n",
    "K = torch.tensor([[[[1, 1, 1, 1]]]])\n",
    "# K.reshape((1,) + K.shape)\n",
    "# K = torch.stack((K, K + 1, K + 2), 0)\n",
    "# print(K)\n",
    "\n",
    "# 2.zip只会最多降低一维\n",
    "X = torch.tensor([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n",
    "Y = torch.tensor([[2], [1], [5],[6]])\n",
    "\n",
    "# for x, y in zip(X, Y):\n",
    "   # print(f'{x} and {y}')\n",
    "# 3.卷积主要是按位乘，区别matmul\n",
    "# 4. cat与stack区别：续接与新增维度\n",
    "Z = torch.tensor([[[5, 5, 5], [6, 6, 6]]])\n",
    "print(torch.cat([X, Z], 0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型性能与计算性能\n",
    "- 二维卷积 || 深度图--3D卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22 池化层\n",
    "## 池化层\n",
    "- 卷积层-位置敏感\n",
    " - 垂直边缘\n",
    " - 平移不变性\n",
    " \n",
    "- 二维最大池化\n",
    "  - 2 * 2最大池化可以容一像素移位\n",
    "  - 填充，步幅和多个通道\n",
    "     - 没有课学习参数\n",
    "     - 每个输入通道应用池化层以获得相应的输出通道\n",
    "     - 输出通道数=输入通道数\n",
    "- 平均池化层\n",
    "   - 最大：每个窗口最强模式信号\n",
    "\n",
    "## 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import d2l.torch as d2l\n",
    "\n",
    "# 实现池化层的正向传播\n",
    "def pool2d(X, poolsize, mod='max'):\n",
    "    h, w = poolsize.shape()\n",
    "    Y = torch.zeros(X.shape[0] - h + 1, X.shape[1] - w + 1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mod == 'max':\n",
    "                Y[i, j] = X[i:i + h, j:j + w].max()\n",
    "            elif mod == 'avg':\n",
    "                Y[i, j] = X[i:i + h, j:j + w].mean()             \n",
    "    return Y\n",
    "# 填充和步幅\n",
    "# 注意参数padding=1是指两侧都添加了，因此是+2\n",
    "X = torch.arange(16, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "pool2d = nn.MaxPool2d(3)   ##注意默认的padding与stride值\n",
    "# pool2d(X)\n",
    "\n",
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "\n",
    "pool2d = nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23 经典卷积神经网络LeNet\n",
    "## LeNet\n",
    "- 手写的数字识别\n",
    "- MNIST\n",
    "- 早期成功NN\n",
    "- 总结：\n",
    "  - 先使用卷积层学习图片空间信息\n",
    "  - 然后使用全连接层来转换到类别空间\n",
    "  \n",
    "## 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import d2l.torch as d2l\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 1, 28, 28)  # 批量数，通道数，28*28\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    Reshape(),\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), #窗口不重叠\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "\n",
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "net = net(X)\n",
    " for layer in net:\n",
    "     #X = layer(X)\n",
    "     print(layer._class_._name_, 'output: \\t', X.shape) \n",
    "## (1,1,28,28)--(1,6,28,28)--(1,6,14,14) \n",
    "## --(1,16,10,10)--(1,16,5,5)\n",
    "## --(16*5*5, 120)--(120, 84)--(84,10)\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    \n",
    "    metric = d2l.Accumulator(2)\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(X, list):\n",
    "            X = [x.to(device) for x in X]\n",
    "        else\n",
    "            X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        metric.add(d2l.accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "\n",
    "# 在GPU上训练的修改：\n",
    "net.to(device)\n",
    "X, y = X.to(device), y.to(device)\n",
    "test_acc = evaluate_accuracy_gpu(net, test_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不断压缩，提升通道维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24 深度卷积神经网络 AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27 含并行连结的网络 GoogLeNet/Inception V3\n",
    "\n",
    "- 1 * 1 卷积核：改变通道数\n",
    "- 卷积核：3 * 3, padding=1; 5 * 5, padding=2(两侧)\n",
    "\n",
    "#### Inception块：\n",
    "- concat: 维度不变，将四种都连接在一起\n",
    "- 每条路上通道数不同，\n",
    "   - 1 * 1 卷积，抽取信息与降低通道数\n",
    "   - 64：128：32：32\n",
    "- 更少的参数设计和复杂度提升\n",
    "\n",
    "#### GoogLeNet\n",
    "- stage：高宽减半\n",
    "<img src=\"https://s2.loli.net/2022/02/13/zS2Ki6Imx3dl8Nb.png\" width=70%>\n",
    "- 通道分配，输出通道增加\n",
    "\n",
    "#### Inception 有各种后续变种\n",
    "- BN(v2) batch normalization\n",
    "- V3 修改了Inception块\n",
    "   - 替换卷积层\n",
    "   - kernel修改\n",
    "   - 更深\n",
    "- V4 使用残差连接\n",
    "\n",
    "#### 总结：\n",
    "- 四条有不同超参数的卷积层和池化层的路来抽取不同信息\n",
    "\n",
    "## QS\n",
    "- 计算量\n",
    "- 经典模型：\n",
    "   - 修改通道数，降低计算量\n",
    "   - 输入输出拉长/拉宽等修改\n",
    "- 核尺度修改：3 * 3-->1 * 3 + 3 * 1 可以降低计算量\n",
    "- DensNet与全连接相同/Flatten：保留通道数，其它展开\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28 批量归一化\n",
    "## 批量归一化\n",
    "- 数据在底部：训练较慢-->收敛变慢（底部：纹理等；顶部：特征等）\n",
    "- 固定mini-batch中方差和均值\n",
    "  - 固定方差和均值\n",
    "  - 额外调整（可学习参数）\n",
    "- 线性变化，必须在激活函数前\n",
    "   - 全连接：作用在特征维 ---- (以列进行)\n",
    "   - 卷积层：作用在通道维\n",
    "\n",
    "- 干什么\n",
    "   - 每个小批量加入噪音控制模型复杂度(?)--随机偏移缩放\n",
    "- sum：\n",
    "   - 固定小批量中的方差和均值，然后学习出合适的偏移和缩放\n",
    "   - 加快收敛，不改变模型精度\n",
    "\n",
    "## 代码\n",
    "### 从0开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import d2l.torch as d2l\n",
    "\n",
    "#----------------函数实现----------------\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum): ## moving_XX 全局的参照\n",
    "    if not torch.is_grad_enabled():\n",
    "        X_hat = gamma * (X- moving_mean) / (torch.sqrt(moving_var) + eps)\n",
    "    assert len(shape(X)) not in (2, 4)\n",
    "    else:\n",
    "        if len(shape(X)) == 2:\n",
    "            mean = X.mean(dim=0, keepdim=True)\n",
    "            var = ((X - mean)**2).mean(dim=0, keepdim=True)\n",
    "        else:\n",
    "            mean = X.mean(dim=(0,2,3), keepdim=True)\n",
    "            var = ((X - mean)**2).mean(dim=(0,2,3), keepdim=True)\n",
    "        X_hat = (X - mean) / (var + eps)\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta \n",
    "    return Y, moving_mean, moving_var\n",
    "       \n",
    "\"\"\"问题\"\"\"\n",
    "# 1.没有处理全局参数\n",
    "# 2.自行求解方差均值，随后再*统一*求更新后的数据\n",
    "# 3.要用现成的API：求平均值别用除法！\n",
    "# 4.出现除法：分母一定不能是0！所以要 +eps\n",
    "\n",
    "\n",
    "#-------------归一化层实现--------------\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, nums_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, nums_features)\n",
    "        else:\n",
    "            shape = (1, nums_features, 1, 1)\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean.to(torch.cuda)\n",
    "            self.moving_var.to(torch.cudaa)\n",
    "              \n",
    "        Y, moving_mean, moving_var = batch_norm(X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "    \n",
    "\"\"\"问题\"\"\"\n",
    "# 1.传入函数的参数都没self  // nn.Module传入类\n",
    "# 2.参数是否需要迭代梯度//参数所使用的设备cuda等设置\n",
    "\n",
    "\n",
    "#-----------LeNet模型实现-----------------\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
    "    nn.Sigmoid(), nn.MaxPool2dx(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4),\n",
    "    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "\"\"\"问题\"\"\"\n",
    "# 1.batchnorm传入参数\n",
    "\n",
    "\n",
    "#------------利用现有的BatchNorm--------\n",
    "nn.BatchNorm2d(6)   # 输入特征数\n",
    "nn.BatchNorm1d(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 归一化(normalization) 标准化 正则化 [参考链接](https://zhuanlan.zhihu.com/p/29957294)\n",
    "- BN主要用在深层网络 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [1, 2, 3]], dtype=float)\n",
    "print(x.mean(dim=0, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29 ResNet残差网络\n",
    "\n",
    "## ResNet\n",
    "- 加更多层\n",
    "- 残差块\n",
    "  - 残差块加入快速通道来得到$f(x)=x+g(x)$结构\n",
    "- 不同的残差块\n",
    "- ResNet块\n",
    "  - 高宽减半，stride=2\n",
    "  - 高宽不变\n",
    "- ResNet架构\n",
    "\n",
    "- ***通常架构：***\n",
    "   - 5个stage\n",
    "   - 1: 7×7 conv + BN + 3×3 pool\n",
    "   - 后4：VGG GoogleNet ResNet\n",
    "- sum:\n",
    "   - 使得深层网络更容易训练\n",
    "\n",
    "\n",
    "## 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, use_1×1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        \n",
    "        if use_1×1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "            \n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(X))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
